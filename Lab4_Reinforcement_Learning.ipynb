{
  "cells": [
    {
      "cell_type": "code",
      "id": "UKys5BfIumG7i2W2BePUPQuI",
      "metadata": {
        "tags": [],
        "id": "UKys5BfIumG7i2W2BePUPQuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70de2434-990f-4165-ebac-4cf372e36132"
      },
      "source": [
        "! pip install gymnasium"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchvision torchaudioy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6VBCnQwzpeC",
        "outputId": "19e6f298-8320-41e9-9cf5-83f3c5d2b577"
      },
      "id": "H6VBCnQwzpeC",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.0+cu121\n",
            "Uninstalling torch-2.5.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.5.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torch-2.5.0+cu121\n",
            "Found existing installation: torchvision 0.20.0+cu121\n",
            "Uninstalling torchvision-0.20.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision-0.20.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libcudart.7ec1eba6.so.12\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libnvjpeg.f00ca762.so.12\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libwebp.4a54d2c8.so.4\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision.libs/libz.e05c5b49.so.1\n",
            "    /usr/local/lib/python3.10/dist-packages/torchvision/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torchvision-0.20.0+cu121\n",
            "Found existing installation: torchaudio 2.5.0+cu121\n",
            "Uninstalling torchaudio-2.5.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio-2.5.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torio/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torchaudio-2.5.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T9OWX9d19d0",
        "outputId": "0e511383-599d-4405-a809-1e2a8cc0f4e1"
      },
      "id": "3T9OWX9d19d0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cartpole Environment"
      ],
      "metadata": {
        "id": "CHnBOdVMArBy"
      },
      "id": "CHnBOdVMArBy"
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# Super Simple Neural Network for DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "target_model = DQN(state_dim, action_dim)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "replay_buffer = deque(maxlen=10000)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "batch_size = 32\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "update_target_every = 100\n",
        "\n",
        "# Helper functions\n",
        "def select_action(state):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_model(next_states).max(1)[0]\n",
        "        targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create a directory for videos if it doesn't exist\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "rewards_list = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        terminal = done or truncated\n",
        "        replay_buffer.append((state, action, reward, next_state, terminal))\n",
        "        train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if terminal:\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "    global epsilon\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Update target model\n",
        "    if episode % update_target_every == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # Generate video every 250 episodes\n",
        "    if episode % 250 == 0:\n",
        "        frames = []\n",
        "        state, info = env.reset()\n",
        "        for _ in range(500):  # Increased to 500 frames\n",
        "            frames.append(env.render())\n",
        "            action = select_action(state)\n",
        "            next_state, _, done, truncated, _ = env.step(action)\n",
        "            state = next_state\n",
        "            if done or truncated:\n",
        "                break\n",
        "        video_path = f\"videos/cartpole_episode_{episode}.mp4\"\n",
        "        imageio.mimsave(video_path, frames, fps=60)  # Increased to 60 fps\n",
        "        print(f\"Video saved: {video_path}\")\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(rewards_list[-50:])\n",
        "        print(f\"Episode {episode}, Reward: {total_reward}, Avg Reward (last 50): {avg_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9DrEJjyx-e",
        "outputId": "a7c2e33f-0293-4600-ff0e-f90b2e11281b"
      },
      "id": "3f9DrEJjyx-e",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3ccd782be44e>:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.FloatTensor(states)\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/cartpole_episode_0.mp4\n",
            "Episode 0, Reward: 59.0, Avg Reward (last 50): 59.0\n",
            "Episode 50, Reward: 11.0, Avg Reward (last 50): 17.66\n",
            "Episode 100, Reward: 17.0, Avg Reward (last 50): 14.44\n",
            "Episode 150, Reward: 13.0, Avg Reward (last 50): 12.48\n",
            "Episode 200, Reward: 12.0, Avg Reward (last 50): 11.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/cartpole_episode_250.mp4\n",
            "Episode 250, Reward: 15.0, Avg Reward (last 50): 16.54\n",
            "Episode 300, Reward: 31.0, Avg Reward (last 50): 20.58\n",
            "Episode 350, Reward: 11.0, Avg Reward (last 50): 41.56\n",
            "Episode 400, Reward: 71.0, Avg Reward (last 50): 37.46\n",
            "Episode 450, Reward: 161.0, Avg Reward (last 50): 112.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/cartpole_episode_500.mp4\n",
            "Episode 500, Reward: 143.0, Avg Reward (last 50): 99.46\n",
            "Episode 550, Reward: 254.0, Avg Reward (last 50): 220.38\n",
            "Episode 600, Reward: 383.0, Avg Reward (last 50): 254.04\n",
            "Episode 650, Reward: 195.0, Avg Reward (last 50): 238.92\n",
            "Episode 700, Reward: 342.0, Avg Reward (last 50): 247.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/cartpole_episode_750.mp4\n",
            "Episode 750, Reward: 244.0, Avg Reward (last 50): 244.84\n",
            "Episode 800, Reward: 178.0, Avg Reward (last 50): 239.08\n",
            "Episode 850, Reward: 119.0, Avg Reward (last 50): 137.24\n",
            "Episode 900, Reward: 123.0, Avg Reward (last 50): 121.02\n",
            "Episode 950, Reward: 128.0, Avg Reward (last 50): 114.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
        "env._max_episode_steps = 1000  # Extend episode length\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "target_model = DQN(state_dim, action_dim)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
        "replay_buffer = deque(maxlen=1000000)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "update_target_every = 1000\n",
        "\n",
        "def select_action(state):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "def custom_reward(state, next_state, action, reward):\n",
        "    # Reward shaping\n",
        "    position = next_state[0]\n",
        "    velocity = next_state[1]\n",
        "    reward += np.exp(position * 5)  # Reward for moving right\n",
        "    if position >= 0.5:\n",
        "        reward += 100  # Bonus for reaching the goal\n",
        "    if position > state[0] and action == 2:  # Moving right\n",
        "        reward += 1\n",
        "    return reward\n",
        "\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Double DQN\n",
        "    current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "    next_actions = model(next_states).max(1)[1].unsqueeze(1)\n",
        "    next_q_values = target_model(next_states).gather(1, next_actions).squeeze()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values, targets.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 5000\n",
        "rewards_list = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "\n",
        "    while True:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        reward = custom_reward(state, next_state, action, reward)\n",
        "        terminal = done or truncated\n",
        "        replay_buffer.append((state, action, reward, next_state, terminal))\n",
        "\n",
        "        if step % 4 == 0:\n",
        "            train()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "        if terminal:\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    global epsilon\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Update target model\n",
        "    if episode % update_target_every == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(rewards_list[-100:])\n",
        "        print(f\"Episode {episode}, Avg Reward (last 100): {avg_reward}\")\n",
        "\n",
        "    # Check for solving condition\n",
        "    if avg_reward > -110:\n",
        "        print(f\"MountainCar-v0 solved in {episode} episodes!\")\n",
        "        break\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SitYE_StLRPp",
        "outputId": "a9383f74-f8e0-4d88-cfa8-86a1c581ec35"
      },
      "id": "SitYE_StLRPp",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Avg Reward (last 100): -736.4163371710349\n",
            "Episode 50, Avg Reward (last 100): -577.1948536722937\n",
            "Episode 100, Avg Reward (last 100): -299.55248282181856\n",
            "Episode 150, Avg Reward (last 100): 14.485462199432039\n",
            "MountainCar-v0 solved in 150 episodes!\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved Neural Network for Double DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "target_model = DQN(state_dim, action_dim)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "replay_buffer = deque(maxlen=1000000)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "epsilon_min = 1e-4\n",
        "epsilon_decay = 0.99\n",
        "update_target_every = 1000\n",
        "\n",
        "# Helper functions\n",
        "def select_action(state):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Double DQN\n",
        "    current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "    next_actions = model(next_states).max(1)[1].unsqueeze(1)\n",
        "    next_q_values = target_model(next_states).gather(1, next_actions).squeeze()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values, targets.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create a directory for videos if it doesn't exist\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "rewards_list = []\n",
        "avg_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "\n",
        "    while True:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        terminal = done or truncated\n",
        "        replay_buffer.append((state, action, reward, next_state, terminal))\n",
        "\n",
        "        if step % 4 == 0:\n",
        "            train()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "        if terminal:\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "    avg_reward = np.mean(rewards_list[-100:])\n",
        "    avg_rewards.append(avg_reward)\n",
        "\n",
        "    global epsilon\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Update target model\n",
        "    if episode % update_target_every == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # Generate video every 500 episodes\n",
        "    if episode % 500 == 0:\n",
        "        frames = []\n",
        "        state, info = env.reset()\n",
        "        for _ in range(500):\n",
        "            frames.append(env.render())\n",
        "            action = select_action(state)\n",
        "            next_state, _, done, truncated, _ = env.step(action)\n",
        "            state = next_state\n",
        "            if done or truncated:\n",
        "                break\n",
        "        video_path = f\"videos/acrobot_episode_{episode}.mp4\"\n",
        "        imageio.mimsave(video_path, frames, fps=60)\n",
        "        print(f\"Video saved: {video_path}\")\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode}, Reward: {total_reward}, Avg Reward (last 100): {avg_reward}\")\n",
        "\n",
        "    # Check for solving condition\n",
        "    if avg_reward > -92.95:\n",
        "        print(f\"Acrobot-v1 solved in {episode} episodes!\")\n",
        "        break\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QjEqJU2Amy",
        "outputId": "17edac7d-d385-494c-ede3-4344b9e2c5a5"
      },
      "id": "p2QjEqJU2Amy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_0.mp4\n",
            "Episode 0, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 50, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 100, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 150, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 200, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 250, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 300, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 350, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 400, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 450, Reward: -500.0, Avg Reward (last 100): -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_500.mp4\n",
            "Episode 500, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 550, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 600, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 650, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 700, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 750, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 800, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 850, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 900, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 950, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved Neural Network for Double DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize environment and model\n",
        "env = gym.make(\"Acrobot-v1\", render_mode='rgb_array')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = DQN(state_dim, action_dim)\n",
        "target_model = DQN(state_dim, action_dim)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "replay_buffer = deque(maxlen=1000000)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "epsilon_min = 1e-4\n",
        "epsilon_decay = 0.99\n",
        "update_target_every = 1000\n",
        "\n",
        "# Helper functions\n",
        "def select_action(state):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "def train():\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Double DQN\n",
        "    current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "    next_actions = model(next_states).max(1)[1].unsqueeze(1)\n",
        "    next_q_values = target_model(next_states).gather(1, next_actions).squeeze()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values, targets.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create a directory for videos if it doesn't exist\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 5000\n",
        "rewards_list = []\n",
        "avg_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "\n",
        "    while True:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        terminal = done or truncated\n",
        "        replay_buffer.append((state, action, reward, next_state, terminal))\n",
        "\n",
        "        if step % 4 == 0:\n",
        "            train()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "\n",
        "        if terminal:\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "    avg_reward = np.mean(rewards_list[-100:])\n",
        "    avg_rewards.append(avg_reward)\n",
        "\n",
        "    global epsilon\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Update target model\n",
        "    if episode % update_target_every == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # Generate video every 500 episodes\n",
        "    if episode % 500 == 0:\n",
        "        frames = []\n",
        "        state, info = env.reset()\n",
        "        for _ in range(500):\n",
        "            frames.append(env.render())\n",
        "            action = select_action(state)\n",
        "            next_state, _, done, truncated, _ = env.step(action)\n",
        "            state = next_state\n",
        "            if done or truncated:\n",
        "                break\n",
        "        video_path = f\"videos/acrobot_episode_{episode}.mp4\"\n",
        "        imageio.mimsave(video_path, frames, fps=60)\n",
        "        print(f\"Video saved: {video_path}\")\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode}, Reward: {total_reward}, Avg Reward (last 100): {avg_reward}\")\n",
        "\n",
        "    # Check for solving condition\n",
        "    if avg_reward > -92.95:\n",
        "        print(f\"Acrobot-v1 solved in {episode} episodes!\")\n",
        "        break\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FtAOrPQ9A1d",
        "outputId": "b467d3ec-9e8a-485d-869f-ded2bdb9ab05"
      },
      "id": "6FtAOrPQ9A1d",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_0.mp4\n",
            "Episode 0, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 50, Reward: -500.0, Avg Reward (last 100): -499.7450980392157\n",
            "Episode 100, Reward: -500.0, Avg Reward (last 100): -499.87\n",
            "Episode 150, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 200, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 250, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 300, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 350, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 400, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 450, Reward: -500.0, Avg Reward (last 100): -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_500.mp4\n",
            "Episode 500, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 550, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 600, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 650, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 700, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 750, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 800, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 850, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 900, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 950, Reward: -500.0, Avg Reward (last 100): -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_1000.mp4\n",
            "Episode 1000, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1050, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1100, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1150, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1200, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1250, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1300, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1350, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1400, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1450, Reward: -500.0, Avg Reward (last 100): -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_1500.mp4\n",
            "Episode 1500, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1550, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1600, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1650, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1700, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1750, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1800, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1850, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1900, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 1950, Reward: -500.0, Avg Reward (last 100): -500.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_2000.mp4\n",
            "Episode 2000, Reward: -500.0, Avg Reward (last 100): -500.0\n",
            "Episode 2050, Reward: -500.0, Avg Reward (last 100): -471.58\n",
            "Episode 2100, Reward: -500.0, Avg Reward (last 100): -404.67\n",
            "Episode 2150, Reward: -500.0, Avg Reward (last 100): -407.64\n",
            "Episode 2200, Reward: -264.0, Avg Reward (last 100): -428.31\n",
            "Episode 2250, Reward: -500.0, Avg Reward (last 100): -403.26\n",
            "Episode 2300, Reward: -347.0, Avg Reward (last 100): -418.78\n",
            "Episode 2350, Reward: -500.0, Avg Reward (last 100): -457.26\n",
            "Episode 2400, Reward: -500.0, Avg Reward (last 100): -449.47\n",
            "Episode 2450, Reward: -367.0, Avg Reward (last 100): -432.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_2500.mp4\n",
            "Episode 2500, Reward: -293.0, Avg Reward (last 100): -436.38\n",
            "Episode 2550, Reward: -500.0, Avg Reward (last 100): -439.48\n",
            "Episode 2600, Reward: -500.0, Avg Reward (last 100): -454.45\n",
            "Episode 2650, Reward: -500.0, Avg Reward (last 100): -460.4\n",
            "Episode 2700, Reward: -265.0, Avg Reward (last 100): -458.65\n",
            "Episode 2750, Reward: -500.0, Avg Reward (last 100): -466.25\n",
            "Episode 2800, Reward: -394.0, Avg Reward (last 100): -466.32\n",
            "Episode 2850, Reward: -500.0, Avg Reward (last 100): -460.1\n",
            "Episode 2900, Reward: -265.0, Avg Reward (last 100): -465.41\n",
            "Episode 2950, Reward: -500.0, Avg Reward (last 100): -458.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_3000.mp4\n",
            "Episode 3000, Reward: -500.0, Avg Reward (last 100): -460.6\n",
            "Episode 3050, Reward: -201.0, Avg Reward (last 100): -372.71\n",
            "Episode 3100, Reward: -160.0, Avg Reward (last 100): -243.36\n",
            "Episode 3150, Reward: -166.0, Avg Reward (last 100): -205.71\n",
            "Episode 3200, Reward: -146.0, Avg Reward (last 100): -197.76\n",
            "Episode 3250, Reward: -117.0, Avg Reward (last 100): -211.66\n",
            "Episode 3300, Reward: -152.0, Avg Reward (last 100): -204.43\n",
            "Episode 3350, Reward: -303.0, Avg Reward (last 100): -197.78\n",
            "Episode 3400, Reward: -113.0, Avg Reward (last 100): -200.04\n",
            "Episode 3450, Reward: -500.0, Avg Reward (last 100): -189.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_3500.mp4\n",
            "Episode 3500, Reward: -170.0, Avg Reward (last 100): -194.19\n",
            "Episode 3550, Reward: -181.0, Avg Reward (last 100): -201.97\n",
            "Episode 3600, Reward: -116.0, Avg Reward (last 100): -197.02\n",
            "Episode 3650, Reward: -141.0, Avg Reward (last 100): -190.71\n",
            "Episode 3700, Reward: -164.0, Avg Reward (last 100): -204.74\n",
            "Episode 3750, Reward: -142.0, Avg Reward (last 100): -223.21\n",
            "Episode 3800, Reward: -237.0, Avg Reward (last 100): -209.84\n",
            "Episode 3850, Reward: -209.0, Avg Reward (last 100): -209.68\n",
            "Episode 3900, Reward: -90.0, Avg Reward (last 100): -208.1\n",
            "Episode 3950, Reward: -131.0, Avg Reward (last 100): -223.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_4000.mp4\n",
            "Episode 4000, Reward: -109.0, Avg Reward (last 100): -238.77\n",
            "Episode 4050, Reward: -314.0, Avg Reward (last 100): -221.21\n",
            "Episode 4100, Reward: -470.0, Avg Reward (last 100): -220.55\n",
            "Episode 4150, Reward: -142.0, Avg Reward (last 100): -195.46\n",
            "Episode 4200, Reward: -163.0, Avg Reward (last 100): -179.88\n",
            "Episode 4250, Reward: -113.0, Avg Reward (last 100): -181.06\n",
            "Episode 4300, Reward: -128.0, Avg Reward (last 100): -178.59\n",
            "Episode 4350, Reward: -177.0, Avg Reward (last 100): -189.99\n",
            "Episode 4400, Reward: -186.0, Avg Reward (last 100): -253.61\n",
            "Episode 4450, Reward: -500.0, Avg Reward (last 100): -271.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved: videos/acrobot_episode_4500.mp4\n",
            "Episode 4500, Reward: -332.0, Avg Reward (last 100): -251.65\n",
            "Episode 4550, Reward: -145.0, Avg Reward (last 100): -302.68\n",
            "Episode 4600, Reward: -500.0, Avg Reward (last 100): -310.35\n",
            "Episode 4650, Reward: -500.0, Avg Reward (last 100): -298.53\n",
            "Episode 4700, Reward: -500.0, Avg Reward (last 100): -349.81\n",
            "Episode 4750, Reward: -500.0, Avg Reward (last 100): -423.24\n",
            "Episode 4800, Reward: -148.0, Avg Reward (last 100): -410.43\n",
            "Episode 4850, Reward: -254.0, Avg Reward (last 100): -365.1\n",
            "Episode 4900, Reward: -458.0, Avg Reward (last 100): -363.7\n",
            "Episode 4950, Reward: -500.0, Avg Reward (last 100): -349.37\n",
            "Training completed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "kayla.hoffman079 (Dec 3, 2024, 7:51:02 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}